{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ecefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6707c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dfbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    x = input_img\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c6f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "x = np.divide(x, 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd219aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = len(np.unique(y))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f2a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred_kmeans = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0247df8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5323571428571429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.acc(y, y_pred_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38344dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [x.shape[-1], 500, 500, 2000, 10]\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                           distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "pretrain_epochs = 300\n",
    "batch_size = 256\n",
    "save_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceda3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a051c47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "274/274 [==============================] - 7s 24ms/step - loss: 0.0646\n",
      "Epoch 2/300\n",
      "274/274 [==============================] - 7s 24ms/step - loss: 0.0436\n",
      "Epoch 3/300\n",
      "274/274 [==============================] - 7s 24ms/step - loss: 0.0315\n",
      "Epoch 4/300\n",
      "274/274 [==============================] - 7s 24ms/step - loss: 0.0268\n",
      "Epoch 5/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0242\n",
      "Epoch 6/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0225\n",
      "Epoch 7/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0213\n",
      "Epoch 8/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0204\n",
      "Epoch 9/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0197\n",
      "Epoch 10/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0190\n",
      "Epoch 11/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0185\n",
      "Epoch 12/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0180\n",
      "Epoch 13/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0176\n",
      "Epoch 14/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0172\n",
      "Epoch 15/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0169\n",
      "Epoch 16/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0166\n",
      "Epoch 17/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0163\n",
      "Epoch 18/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0161\n",
      "Epoch 19/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0159\n",
      "Epoch 20/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0157\n",
      "Epoch 21/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0155\n",
      "Epoch 22/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0153\n",
      "Epoch 23/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0151\n",
      "Epoch 24/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0149\n",
      "Epoch 25/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0148\n",
      "Epoch 26/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0147\n",
      "Epoch 27/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0145\n",
      "Epoch 28/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0144\n",
      "Epoch 29/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0143\n",
      "Epoch 30/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0141\n",
      "Epoch 31/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0140\n",
      "Epoch 32/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0139\n",
      "Epoch 33/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0138\n",
      "Epoch 34/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0137\n",
      "Epoch 35/300\n",
      "274/274 [==============================] - 8s 29ms/step - loss: 0.0136\n",
      "Epoch 36/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0135\n",
      "Epoch 37/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0134\n",
      "Epoch 38/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0133\n",
      "Epoch 39/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0133\n",
      "Epoch 40/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0132\n",
      "Epoch 41/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0131\n",
      "Epoch 42/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0130\n",
      "Epoch 43/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0130\n",
      "Epoch 44/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0129\n",
      "Epoch 45/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0128\n",
      "Epoch 46/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0127\n",
      "Epoch 47/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0127\n",
      "Epoch 48/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0126\n",
      "Epoch 49/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0126\n",
      "Epoch 50/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0125\n",
      "Epoch 51/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0124\n",
      "Epoch 52/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0124\n",
      "Epoch 53/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0123\n",
      "Epoch 54/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0123\n",
      "Epoch 55/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0122\n",
      "Epoch 56/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0122\n",
      "Epoch 57/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0121\n",
      "Epoch 58/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0120\n",
      "Epoch 59/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0120\n",
      "Epoch 60/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0120\n",
      "Epoch 61/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0119\n",
      "Epoch 62/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0119\n",
      "Epoch 63/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0118\n",
      "Epoch 64/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0118\n",
      "Epoch 65/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0117\n",
      "Epoch 66/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0117\n",
      "Epoch 67/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0116\n",
      "Epoch 68/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0116\n",
      "Epoch 69/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0116\n",
      "Epoch 70/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0115\n",
      "Epoch 71/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0115\n",
      "Epoch 72/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0114\n",
      "Epoch 73/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0114\n",
      "Epoch 74/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0114\n",
      "Epoch 75/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0113\n",
      "Epoch 76/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0113\n",
      "Epoch 77/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0113\n",
      "Epoch 78/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0112\n",
      "Epoch 79/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0112\n",
      "Epoch 80/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0112\n",
      "Epoch 81/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0111\n",
      "Epoch 82/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0111\n",
      "Epoch 83/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0111\n",
      "Epoch 84/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0110\n",
      "Epoch 85/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0110\n",
      "Epoch 86/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0110\n",
      "Epoch 87/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0109\n",
      "Epoch 88/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0109\n",
      "Epoch 89/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0109\n",
      "Epoch 90/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0109\n",
      "Epoch 91/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0108\n",
      "Epoch 92/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0108\n",
      "Epoch 93/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0108\n",
      "Epoch 94/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0107\n",
      "Epoch 95/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0107\n",
      "Epoch 96/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0107\n",
      "Epoch 97/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0107\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0106\n",
      "Epoch 99/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0106\n",
      "Epoch 100/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0106\n",
      "Epoch 101/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0106\n",
      "Epoch 102/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0105\n",
      "Epoch 103/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0105\n",
      "Epoch 104/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0105\n",
      "Epoch 105/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0105\n",
      "Epoch 106/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0105\n",
      "Epoch 107/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0104\n",
      "Epoch 108/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0104\n",
      "Epoch 109/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0104\n",
      "Epoch 110/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0104\n",
      "Epoch 111/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0103\n",
      "Epoch 112/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0103\n",
      "Epoch 113/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0103\n",
      "Epoch 114/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0103\n",
      "Epoch 115/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0103\n",
      "Epoch 116/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0102\n",
      "Epoch 117/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0102\n",
      "Epoch 118/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0102\n",
      "Epoch 119/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0102\n",
      "Epoch 120/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0102\n",
      "Epoch 121/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0101\n",
      "Epoch 122/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0101\n",
      "Epoch 123/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0101\n",
      "Epoch 124/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0101\n",
      "Epoch 125/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0101\n",
      "Epoch 126/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0100\n",
      "Epoch 127/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0100\n",
      "Epoch 128/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0100\n",
      "Epoch 129/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0100\n",
      "Epoch 130/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0100\n",
      "Epoch 131/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0100\n",
      "Epoch 132/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0099\n",
      "Epoch 133/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0099\n",
      "Epoch 134/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0099\n",
      "Epoch 135/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0099\n",
      "Epoch 136/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0099\n",
      "Epoch 137/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0099\n",
      "Epoch 138/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0098\n",
      "Epoch 139/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0098\n",
      "Epoch 140/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0098\n",
      "Epoch 141/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0098\n",
      "Epoch 142/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0098\n",
      "Epoch 143/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0098\n",
      "Epoch 144/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0097\n",
      "Epoch 145/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0097\n",
      "Epoch 146/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0097\n",
      "Epoch 147/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0097\n",
      "Epoch 148/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0097\n",
      "Epoch 149/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0097\n",
      "Epoch 150/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0096\n",
      "Epoch 151/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0096\n",
      "Epoch 152/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0096\n",
      "Epoch 153/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0096\n",
      "Epoch 154/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0096\n",
      "Epoch 155/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0096\n",
      "Epoch 156/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0096\n",
      "Epoch 157/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0095\n",
      "Epoch 158/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0095\n",
      "Epoch 159/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0095\n",
      "Epoch 160/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0095\n",
      "Epoch 161/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0095\n",
      "Epoch 162/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0095\n",
      "Epoch 163/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0095\n",
      "Epoch 164/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0095\n",
      "Epoch 165/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 166/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 167/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 168/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0094\n",
      "Epoch 169/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 170/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 171/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 172/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 173/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0094\n",
      "Epoch 174/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 175/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 176/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 177/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0093\n",
      "Epoch 178/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 179/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 180/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 181/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0093\n",
      "Epoch 182/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 183/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 184/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 185/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 186/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0092\n",
      "Epoch 187/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0092\n",
      "Epoch 188/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 189/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0092\n",
      "Epoch 190/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0092\n",
      "Epoch 191/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0091\n",
      "Epoch 192/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0091\n",
      "Epoch 193/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0091\n",
      "Epoch 194/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0091\n",
      "Epoch 195/300\n",
      "274/274 [==============================] - 8s 28ms/step - loss: 0.0091\n",
      "Epoch 196/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0091\n",
      "Epoch 197/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0091\n",
      "Epoch 198/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0091\n",
      "Epoch 199/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0091\n",
      "Epoch 200/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 201/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 202/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 203/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0090\n",
      "Epoch 204/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 205/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 206/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 207/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0090\n",
      "Epoch 208/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 209/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 210/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0090\n",
      "Epoch 211/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0089\n",
      "Epoch 212/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 213/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 214/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 215/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 216/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0089\n",
      "Epoch 217/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 218/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 219/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 220/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0089\n",
      "Epoch 221/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0089\n",
      "Epoch 222/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 223/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 224/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 225/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0088\n",
      "Epoch 226/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 227/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 228/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 229/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0088\n",
      "Epoch 230/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 231/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 232/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 233/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0088\n",
      "Epoch 234/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0087\n",
      "Epoch 235/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 236/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 237/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0087\n",
      "Epoch 238/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0087\n",
      "Epoch 239/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 240/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 241/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 242/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 243/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0087\n",
      "Epoch 244/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 245/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0087\n",
      "Epoch 246/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0086\n",
      "Epoch 247/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 248/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 249/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 250/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 251/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 252/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0086\n",
      "Epoch 253/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 254/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 255/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0086\n",
      "Epoch 256/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 257/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0086\n",
      "Epoch 258/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 259/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 260/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0086\n",
      "Epoch 261/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0085\n",
      "Epoch 262/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 263/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0085\n",
      "Epoch 264/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0085\n",
      "Epoch 265/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 266/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 267/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 268/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 269/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 270/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0085\n",
      "Epoch 271/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 272/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0085\n",
      "Epoch 273/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 274/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0085\n",
      "Epoch 275/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 276/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 277/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 278/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 279/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 280/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 281/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0084\n",
      "Epoch 282/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 283/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 284/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 285/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 286/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 287/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 288/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 289/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0084\n",
      "Epoch 290/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0084\n",
      "Epoch 291/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 292/300\n",
      "274/274 [==============================] - 7s 24ms/step - loss: 0.0083\n",
      "Epoch 293/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 294/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 295/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 296/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 297/300\n",
      "274/274 [==============================] - 7s 25ms/step - loss: 0.0083\n",
      "Epoch 298/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0083\n",
      "Epoch 299/300\n",
      "274/274 [==============================] - 7s 26ms/step - loss: 0.0083\n",
      "Epoch 300/300\n",
      "274/274 [==============================] - 7s 27ms/step - loss: 0.0083\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to create file (unable to open file: name = './results/ae_weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30428\\1372341680.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrain_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, callbacks=cb)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/ae_weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[1;32m--> 533\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to create file (unable to open file: name = './results/ae_weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs) #, callbacks=cb)\n",
    "autoencoder.save_weights(save_dir + '/ae_weights.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fc20844",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e6161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab32b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((int(self.n_clusters), int(input_dim)), initializer='glorot_uniform')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d0464b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30428\\2584008830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclustering_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClusteringLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclustering_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30428\\3070913908.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glorot_uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters)(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a81187",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b74504",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9faa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 8000\n",
    "update_interval = 140\n",
    "index_array = np.arange(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38faf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "            nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "            ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion - model convergence\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval.\n",
    "q = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)\n",
    "if y is not None:\n",
    "    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "    loss = np.round(loss, 5)\n",
    "    print('Acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari), ' ; loss=', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=3)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
