{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9799cd82",
   "metadata": {},
   "source": [
    "# Crop Field Siamese with Triplet Loss with Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfac48",
   "metadata": {},
   "source": [
    "This notebook is part of the field boundary detection project. The goal is to evaluate the algorithm using 2D time series of the same h3 hex AND semi-supervised contrastive learning with Graph-based segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf1b3a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "[1. Data Processing](#data_processing)\n",
    "\n",
    "[2. Model Training](#model_training)\n",
    "\n",
    "[3. Segmentation](#segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d094de",
   "metadata": {},
   "source": [
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Install packages required only once\n",
    "install_packages = False\n",
    "\n",
    "if install_packages:\n",
    "    !{sys.executable} -m pip install sklearn\n",
    "    !{sys.executable} -m pip install seaborn\n",
    "    !{sys.executable} -m pip install utm\n",
    "    !{sys.executable} -m pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2098e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn.objects as so\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import folium\n",
    "import h3\n",
    "import uuid\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75990c",
   "metadata": {},
   "source": [
    "## Hyper-parameters and other constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_samples = 16\n",
    "band_features = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "#band_features = ['B02', 'B03', 'B04'] # only RGB spectral\n",
    "number_of_bands = len(band_features)\n",
    "\n",
    "\n",
    "# output vector size\n",
    "width = 20\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "shuffle_buffer = 50\n",
    "num_epochs = 200\n",
    "\n",
    "train_only_on_accurate_samples = True\n",
    "siamese_margin = 10\n",
    "\n",
    "loss_function_distance = 'cosine' # 'cosine' or 'euclidean'\n",
    "\n",
    "# Sample creation parameters\n",
    "sampling_method = 'season' # 'shuffle', 'season' \n",
    "fills_sample_whentimeseries_navailable = False\n",
    "\n",
    "visualization_level = 3\n",
    "# vl = 0 : No visuals at all\n",
    "# vl = 1 : Just visuals that are proportional to results\n",
    "# vl = 2 : Visuals that show results and maps\n",
    "# vl = 3 : All visuals\n",
    "\n",
    "generate_paper_info = False\n",
    "\n",
    "folder = f'report_tls_{width}_{siamese_margin}_{loss_function_distance}_{sampling_method}\\\\'\n",
    "\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except:\n",
    "    print('Folder already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21fee2",
   "metadata": {},
   "source": [
    "<a id='data_processing'></a>\n",
    "\n",
    "# 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143713b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img border=\"5px\" src=\"../res/dataprocessing.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a13970",
   "metadata": {},
   "source": [
    "## Configure the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "positive_samples_folder = Path('D:\\\\173_seeding_harvest_joined_USCA_mini.parquet\\\\positive_samples\\\\hex_index_L3=8348b3fffffffff')\n",
    "negative_samples_folder = Path('D:\\\\173_seeding_harvest_joined_USCA_mini.parquet\\\\negative_samples\\\\hex_index_L3=8348b3fffffffff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b28f63",
   "metadata": {},
   "source": [
    "## Load all parquet files from the configured folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2f60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load positive hexes\n",
    "df_positive = pd.concat(\n",
    "            pd.read_parquet(parquet_file)\n",
    "            for parquet_file in positive_samples_folder.rglob('*.parquet')\n",
    "         )\n",
    "\n",
    "# load negative hexes\n",
    "df_negative = pd.concat(\n",
    "            pd.read_parquet(parquet_file)\n",
    "            for parquet_file in negative_samples_folder.rglob('*.parquet'))\n",
    "\n",
    "print('The shape of loaded positive dataframe before dropping duplicates is:', df_positive.shape)\n",
    "print('The shape of loaded negative dataframe before dropping duplicates is:', df_negative.shape)\n",
    "\n",
    "df_positive = df_positive.drop_duplicates()\n",
    "df_negative = df_negative.drop_duplicates()\n",
    "\n",
    "# create timestamp columns based on scene_id\n",
    "df_positive['timestamp'] = df_positive.scene_id.str[11:26]\n",
    "df_negative['timestamp'] = df_negative.scene_id.str[11:26]\n",
    "\n",
    "print('The shape of loaded positive dataframe before dropping duplicates is:', df_positive.shape)\n",
    "print('The shape of loaded negative dataframe before dropping duplicates is:', df_negative.shape)\n",
    "\n",
    "display(df_positive.head(5))\n",
    "display(df_negative.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebfe5c",
   "metadata": {},
   "source": [
    "## Unique fields in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2a6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_positive.FIELD_OPERATION_GUID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2ee04",
   "metadata": {},
   "source": [
    "## Remove hexes that are represented in both positive and negative datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could also just set it to \"field\" samples, since we know machine was in the \n",
    "# position but let's remove for the sake of class balacing\n",
    "\n",
    "print('In this dataset there are ', df_positive.hex.unique().size, ' different positive hexes')\n",
    "print('In this dataset there are ', df_negative.hex.unique().size, ' different negative hexes')\n",
    "\n",
    "positive_l12_hexes = df_positive.hex.unique()\n",
    "negative_l12_hexes = df_negative.hex.unique()\n",
    "\n",
    "ambiguous_l12_hexes = set(positive_l12_hexes).intersection(negative_l12_hexes)\n",
    "print('There are ', len(ambiguous_l12_hexes), ' hexes labeled both as positive and negative')\n",
    "\n",
    "df_positive = df_positive[~df_positive['hex'].isin(ambiguous_l12_hexes)]\n",
    "df_negative = df_negative[~df_negative['hex'].isin(ambiguous_l12_hexes)]\n",
    "\n",
    "print('In this dataset there are ', df_positive.hex.unique().size, ' different positive hexes')\n",
    "print('In this dataset there are ', df_negative.hex.unique().size, ' different negative hexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization_level > 2:\n",
    "    sns.set(rc={'figure.figsize':(18.7,2.27)})\n",
    "\n",
    "    temporal_samples_count = df_positive.groupby(['hex'])['hex'].count()\n",
    "\n",
    "    print('There are hexes with temporal samples from', temporal_samples_count.min(), 'to', temporal_samples_count.max())\n",
    "\n",
    "    hist = pd.DataFrame(temporal_samples_count) \\\n",
    "            .rename(columns={'hex': 'count'}) \\\n",
    "            .reset_index(level=0) \\\n",
    "            .groupby(['count'])['count'].count()\n",
    "\n",
    "    hist_df = pd.DataFrame(hist) \\\n",
    "              .rename(columns={'count': 'frequency'}) \\\n",
    "              .reset_index(level=0)\n",
    "\n",
    "    #so.Plot(hist_df[\"count\"], hist_df[\"frequency\"]) \\\n",
    "    #    .add(so.Bar()) \\\n",
    "    #    .scale(x=so.Continuous().tick(every=2)) \\\n",
    "    #    .layout(size=(10, 4))\n",
    "\n",
    "    ax = sns.boxplot(x=hist_df[\"count\"])\n",
    "\n",
    "    ax.set_xlabel(\"Number of image dates by hex\",fontsize=30)\n",
    "    ax.tick_params(labelsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548ea3c",
   "metadata": {},
   "source": [
    "## Sentinel 2 bands\n",
    "These are the 12 bands contained in the parque files and their respective meanings\n",
    "\n",
    "<div>\n",
    "<img src=\"../res/sentinel2-bands.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bbcc2",
   "metadata": {},
   "source": [
    "## Show some of the L12 hexes loaded in the folium map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_only_on_accurate_samples:\n",
    "    # Drop hexes from non-conclusive areas\n",
    "    drop_list = np.loadtxt('drop_list.csv', delimiter=',', dtype='str')\n",
    "\n",
    "    df_negative = df_negative[~df_negative['hex'].isin(drop_list)]\n",
    "    df_positive = df_positive[~df_positive['hex'].isin(drop_list)]\n",
    "    \n",
    "    # Treat false negatives\n",
    "    false_negatives = np.loadtxt('false_negatives.csv', delimiter=',', dtype='str')\n",
    "\n",
    "    df_negative = df_negative.assign(label=0)\n",
    "    df_positive = df_positive.assign(label=1)\n",
    "    \n",
    "    df_negative = df_negative.reset_index()\n",
    "    df_positive = df_positive.reset_index()\n",
    "    \n",
    "    # Assign to negatives recognized as false the label 1\n",
    "    df_negative.loc[df_negative['hex'].isin(false_negatives), 'label'] = 1\n",
    "\n",
    "    df_positive = pd.concat([df_positive, df_negative[df_negative.label==1]])\n",
    "    \n",
    "    df_negative = df_negative.drop(df_negative[df_negative.label == 1].index)\n",
    "    print('Trained on manually curated samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde48b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_paper_info:\n",
    "    for index, f in enumerate(df_positive.FIELD_OPERATION_GUID.unique()):\n",
    "        print('F{:02d}'.format(index+1) +\n",
    "              '&'+\n",
    "              f'{df_positive[df_positive.FIELD_OPERATION_GUID == f].hex.nunique():,}' +\n",
    "              '&'+\n",
    "              f'{df_negative[df_negative.FIELD_OPERATION_GUID == f].hex.nunique()}' +\n",
    "              '&'+\n",
    "              f'{df_positive[df_positive.FIELD_OPERATION_GUID == f].hex.count():,}' +\n",
    "              '&'+\n",
    "              f'{df_negative[df_negative.FIELD_OPERATION_GUID == f].hex.count():,}' +\n",
    "              '&'+\n",
    "              '{:.2f}'.format(df_positive[df_positive.FIELD_OPERATION_GUID == f].groupby(\"hex\").hex.count().mean()) +\n",
    "              '&'+\n",
    "              '{:.2f}'.format(df_positive[df_positive.FIELD_OPERATION_GUID == f].groupby(\"hex\").hex.count().std()) +\n",
    "              '\\\\\\\\'\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabce1df",
   "metadata": {},
   "source": [
    "## Display hexes in the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75993a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def display_hexes_map_v2(df, hexes_to_print=50000, filename=None):\n",
    "\n",
    "    h3_hex = h3.h3_to_parent(df.iloc[0]['hex'], 3)\n",
    "\n",
    "    df = df.drop_duplicates(subset='hex', keep=\"first\").head(hexes_to_print)\n",
    "    \n",
    "    min_lat, min_long = 9999999,9999999\n",
    "    max_lat, max_long = -9999999,-9999999\n",
    "    \n",
    "    \n",
    "    for h in df.hex:\n",
    "        p = h3.h3_to_geo(h)\n",
    "        \n",
    "        if p[0] < min_lat:\n",
    "            min_lat = p[0]\n",
    "            \n",
    "        if p[0] > max_lat:\n",
    "            max_lat = p[0]\n",
    "            \n",
    "        if p[1] < min_long:\n",
    "            min_long = p[1]\n",
    "            \n",
    "        if p[1] > max_long:\n",
    "            max_long = p[1]\n",
    "            \n",
    "    bounds = [[min_lat, min_long], [max_lat, max_long]]\n",
    "    print(bounds)\n",
    "\n",
    "    m = folium.Map(\n",
    "                    tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "                    attr = 'Esri',\n",
    "                    name = 'Esri Satellite',\n",
    "                    zoom_start=11,\n",
    "                    fit_bounds=[[-97.7198928194847,26.236073089251086],[-97.66473848550342,26.243580591892144]],\n",
    "                    overlay = False,\n",
    "                    control = True)\n",
    "\n",
    "    # Print h3 hex\n",
    "    geometry = { \"type\" : \"Polygon\", \"coordinates\": [h3.h3_to_geo_boundary(h=h3_hex,geo_json=True)]}\n",
    "    geo_j = folium.GeoJson(data=geometry, style_function=lambda x: {'fillColor': 'white', 'color': 'white', 'weight': 0.5})\n",
    "    geo_j.add_to(m)\n",
    "\n",
    "    # Print positive samples\n",
    "    for index, row in df.iterrows():\n",
    "        geometry = { \"type\" : \"Polygon\", \"coordinates\": [h3.h3_to_geo_boundary(h=row['hex'],geo_json=True)]}\n",
    "        if row['label'] == 1:\n",
    "            geo_j = folium.GeoJson(data=geometry, style_function=lambda x: {'fillColor': 'yellow', 'color': 'green', 'weight': 0.5})\n",
    "        elif row['label'] == 0:\n",
    "            geo_j = folium.GeoJson(data=geometry, style_function=lambda x: {'fillColor': 'red', 'color': 'black', 'weight': 0.5})\n",
    "        else:\n",
    "            geo_j = folium.GeoJson(data=geometry, style_function=lambda x: {'fillColor': 'black', 'color': 'black', 'weight': 0.5})\n",
    "        folium.Popup(str(row['FIELD_OPERATION_GUID']) + ' ' + str(row['hex'])).add_to(geo_j)\n",
    "        geo_j.add_to(m)\n",
    "\n",
    "\n",
    "    m.fit_bounds(bounds)\n",
    "    \n",
    "    display(m)\n",
    "    \n",
    "    if filename is not None:\n",
    "        img_data = m._to_png(5)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        img.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf640c8",
   "metadata": {},
   "source": [
    "# Visualize the samples in the space reduced to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9232273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_test(dfp, dfn, sampling_method = 'shuffle', fills_sample_whentimeseries_navailable = False):\n",
    "\n",
    "    # Keep only [temporal samples] samples \n",
    "#    dfp = dfp.sort_values(by=['hex','timestamp','B12'])\n",
    "#    dfp = dfp.groupby('hex').head(temporal_samples)\n",
    "\n",
    "#    dfn = dfn.sort_values(by=['hex','timestamp','B12'])\n",
    "#    dfn = dfn.groupby('hex').head(temporal_samples)\n",
    "    \n",
    "\n",
    "    # Associate labels picked manually\n",
    "    dfn = dfn.assign(label=0)\n",
    "    dfp = dfp.assign(label=1)\n",
    "    df = pd.concat([dfp, dfn], axis=0)\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "    print('Number of total rows:' +  str(len(df.index)))\n",
    "    print('Number of rows in positive dataset:' +  str(len(dfp.index)))\n",
    "    print('Number of rows in negative dataset:' +  str(len(dfp.index)))\n",
    "    print('Number of unique positive hexes:' +  str(dfp.hex.nunique()))\n",
    "    print('Number of unique negative hexes:' +  str(dfp.hex.nunique()))\n",
    "\n",
    "\n",
    "    df[band_features] = StandardScaler().fit_transform(df[band_features])\n",
    "\n",
    "    # Organize the 2D samples in numpy arrays\n",
    "    sample = np.zeros((temporal_samples, number_of_bands), dtype=np.float64)\n",
    "    X_array = np.empty((0, temporal_samples, number_of_bands),  dtype=np.float64)\n",
    "\n",
    "    labels = []\n",
    "    hexes = []\n",
    "    fop = []\n",
    "    timestamp_tracking = []\n",
    "    timestamp_sample = []\n",
    "\n",
    "    sub_index = 0\n",
    "    count = 0;\n",
    "    display(df.shape)\n",
    "\n",
    "    \n",
    "    for h in df.hex.unique():\n",
    "        pct_complete = count/df.shape[0] * 100\n",
    "        print('Sampling {0:.2f}'.format(pct_complete) + '%', end='\\r')\n",
    "        count = count + 1\n",
    "        \n",
    "        hexdf = df[df.hex == h]\n",
    "        \n",
    "        display(hexdf)\n",
    "        \n",
    "        for index, row in hexdf.iterrows():\n",
    "            # fill the band values in a temporal row\n",
    "            #for idx,b in enumerate(band_features):\n",
    "            #    sample[sub_index][idx] = row[b]    \n",
    "\n",
    "\n",
    "            timestamp_sample.append(row.timestamp)\n",
    "\n",
    "            # increment row number \n",
    "            sub_index = sub_index + 1\n",
    "\n",
    "            # if reached last row of temporal samples, increment to next sample\n",
    "            if sub_index == temporal_samples:\n",
    "                if sampling_method == 'season':\n",
    "                    try:\n",
    "                        timestamp_sample_aux = []\n",
    "                        winter_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 1 and int(s[4:6]) <= 3]\n",
    "                        winter_samples = np.random.choice(winter_samples, size=4)\n",
    "                        #print('wsamples', winter_samples)\n",
    "                        winter_array = np.array(sample)[winter_samples]\n",
    "                        timestamp_sample_aux.extend([timestamp_sample[i] for i in winter_samples])\n",
    "\n",
    "                        spring_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 4 and int(s[4:6]) <= 6]\n",
    "                        spring_samples = np.random.choice(spring_samples, size=4)\n",
    "                        #print('spamples', sprint_samples)\n",
    "                        spring_array = np.array(sample)[spring_samples]\n",
    "                        timestamp_sample_aux.extend([timestamp_sample[i] for i in spring_samples])\n",
    "\n",
    "                        summer_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 7 and int(s[4:6]) <= 9]\n",
    "\n",
    "                        summer_samples = np.random.choice(summer_samples, size=4)\n",
    "\n",
    "                        summer_array = np.array(sample)[summer_samples]\n",
    "                        timestamp_sample_aux.extend([timestamp_sample[i] for i in summer_samples])\n",
    "\n",
    "                        fall_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 10 and int(s[4:6]) <= 12]\n",
    "                        fall_samples = np.random.choice(fall_samples, size=4)\n",
    "                        #print('fall', fall_samples)\n",
    "                        fall_array = np.array(sample)[fall_samples]\n",
    "                        timestamp_sample_aux.extend([timestamp_sample[i] for i in fall_samples])\n",
    "                        timestamp_sample = timestamp_sample_aux\n",
    "\n",
    "                        sample = np.concatenate((winter_array, spring_array, summer_array, fall_array), axis=0)\n",
    "                    except Exception as e:\n",
    "                        print('Exception while creating season sample', e)\n",
    "                        print('summer', summer_samples)\n",
    "                        pass\n",
    "\n",
    "                X_array = np.append(X_array, [sample], axis=0)\n",
    "                labels.append(row.label)\n",
    "                hexes.append(row.hex)\n",
    "                fop.append(row.FIELD_OPERATION_GUID)\n",
    "                timestamp_tracking.append(timestamp_sample.copy())            \n",
    "\n",
    "                timestamp_sample.clear()\n",
    "                sub_index = 0\n",
    "\n",
    "\n",
    "    sorted_indices = np.argsort(hexes)\n",
    "    hexes = np.array(hexes)[sorted_indices]\n",
    "    labels = np.array(labels)[sorted_indices]\n",
    "    X_array = X_array[sorted_indices]\n",
    "    fop = np.array(fop)[sorted_indices]\n",
    "    timestamp_tracking = np.array(timestamp_tracking)[sorted_indices]\n",
    "    df = df.sort_values(by=['hex'])\n",
    "\n",
    "    return df, X_array, labels, hexes, fop, timestamp_tracking\n",
    "\n",
    "df, X_array, labels, hexes, fop, timestamp_tracking = generate_samples_test(df_positive, df_negative, sampling_method=sampling_method)\n",
    "\n",
    "print(X_array.shape)\n",
    "print(len(labels))\n",
    "print(len(hexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502d5a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(dfp, dfn, sampling_method = 'shuffle', fills_sample_whentimeseries_navailable = False):\n",
    "\n",
    "    # Keep only [temporal samples] samples \n",
    "    dfp = dfp.sort_values(by=['hex','timestamp','B12'])\n",
    "    dfp = dfp.groupby('hex').head(temporal_samples * 10)\n",
    "\n",
    "    dfn = dfn.sort_values(by=['hex','timestamp','B12'])\n",
    "    dfn = dfn.groupby('hex').head(temporal_samples * 10)\n",
    "    \n",
    "    #display(dfn.head(temporal_samples))\n",
    "\n",
    "    # Associate labels picked manually\n",
    "    dfn = dfn.assign(label=0)\n",
    "    dfp = dfp.assign(label=1)\n",
    "    df = pd.concat([dfp, dfn], axis=0)\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df['timestamp_fixed'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "    print('Number of total rows:' +  str(len(df.index)))\n",
    "    print('Number of rows in positive dataset:' +  str(len(dfp.index)))\n",
    "    print('Number of rows in negative dataset:' +  str(len(dfp.index)))\n",
    "    print('Number of unique positive hexes:' +  str(dfp.hex.nunique()))\n",
    "    print('Number of unique negative hexes:' +  str(dfp.hex.nunique()))\n",
    "\n",
    "\n",
    "    if fills_sample_whentimeseries_navailable:\n",
    "        grouped_df = df.groupby(['hex'])\n",
    "\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        # Loop through each group in the grouped DataFrame\n",
    "        count = 0;\n",
    "        hexes_count = df['hex'].nunique()\n",
    "        for group_name, group_data in grouped_df:\n",
    "            count = count + 1\n",
    "            pct_complete = count/hexes_count * 100\n",
    "            print('Packing {0:.2f}'.format(pct_complete) + '% (' + str(count) + '/' + str(hexes_count) + ')', end='\\r')\n",
    "\n",
    "            # Check if the group has more than 5 rows\n",
    "            if len(group_data) > temporal_samples:\n",
    "                # If yes, randomly sample 5 rows and add them to the new DataFrame\n",
    "                new_rows = group_data.sample(n=temporal_samples, replace=False)\n",
    "                new_df = pd.concat([new_df, new_rows])\n",
    "            else:\n",
    "                # If no, repeat the existing rows until there are 5 rows and add them to the new DataFrame\n",
    "                num_repeats = temporal_samples // len(group_data) + 1\n",
    "                repeated_rows = pd.concat([group_data] * num_repeats, ignore_index=True)\n",
    "                new_rows = repeated_rows.iloc[:temporal_samples]\n",
    "                new_df = pd.concat([new_df, new_rows])\n",
    "\n",
    "        # View the new DataFrame\n",
    "        display(new_df)\n",
    "\n",
    "        df = new_df.copy()\n",
    "\n",
    "\n",
    "    df[band_features] = StandardScaler().fit_transform(df[band_features])\n",
    "\n",
    "    # Organize the 2D samples in numpy arrays\n",
    "    sample = np.zeros((temporal_samples, number_of_bands), dtype=np.float64)\n",
    "    X_array = np.empty((0, temporal_samples, number_of_bands),  dtype=np.float64)\n",
    "\n",
    "    labels = []\n",
    "    hexes = []\n",
    "    fop = []\n",
    "    timestamp_tracking = []\n",
    "    timestamp_sample = []\n",
    "    errors = 0\n",
    "\n",
    "    row_index = 0\n",
    "    count = 0;\n",
    "    display(df.shape)\n",
    "    \n",
    "    # Loop over the rows\n",
    "    for index, row in df.iterrows():      \n",
    "        pct_complete = count/df.shape[0] * 100\n",
    "        print('Sampling {0:.2f}'.format(pct_complete) + '%', end='\\r')\n",
    "        count = count + 1\n",
    "\n",
    "        # fill the band values in a temporal row\n",
    "        for idx,b in enumerate(band_features):\n",
    "            sample[row_index][idx] = row[b]\n",
    "\n",
    "        timestamp_sample.append(row.timestamp)\n",
    "        \n",
    "        print(row)\n",
    "        break\n",
    "\n",
    "        # increment row number \n",
    "        row_index = row_index + 1\n",
    "\n",
    "        # if reached last row of temporal samples, increment to next sample\n",
    "        if row_index == temporal_samples:\n",
    "\n",
    "            if sampling_method == 'shuffle':\n",
    "                shuffler = np.random.permutation(sample.shape[0])\n",
    "                sample = sample[shuffler]\n",
    "                timestamp_sample = list(np.array(timestamp_sample)[shuffler])\n",
    "\n",
    "            elif sampling_method == 'season':\n",
    "                try:\n",
    "                    timestamp_sample_aux = []\n",
    "                    winter_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 1 and int(s[4:6]) <= 3]\n",
    "                    winter_samples = np.random.choice(winter_samples, size=4)\n",
    "                    winter_array = np.array(sample)[winter_samples]\n",
    "                    timestamp_sample_aux.extend([timestamp_sample[i] for i in winter_samples])\n",
    "\n",
    "                    spring_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 4 and int(s[4:6]) <= 6]\n",
    "                    spring_samples = np.random.choice(spring_samples, size=4)\n",
    "                    spring_array = np.array(sample)[spring_samples]\n",
    "                    timestamp_sample_aux.extend([timestamp_sample[i] for i in spring_samples])\n",
    "\n",
    "                    summer_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 7 and int(s[4:6]) <= 9]\n",
    "                    summer_samples = np.random.choice(summer_samples, size=4)\n",
    "                    summer_array = np.array(sample)[summer_samples]\n",
    "                    timestamp_sample_aux.extend([timestamp_sample[i] for i in summer_samples])\n",
    "\n",
    "                    fall_samples = [i for i, s in enumerate(timestamp_sample) if int(s[4:6]) >= 10 and int(s[4:6]) <= 12]\n",
    "                    fall_samples = np.random.choice(fall_samples, size=4)\n",
    "                    fall_array = np.array(sample)[fall_samples]\n",
    "                    timestamp_sample_aux.extend([timestamp_sample[i] for i in fall_samples])\n",
    "                    timestamp_sample = timestamp_sample_aux\n",
    "\n",
    "                    sample = np.concatenate((winter_array, spring_array, summer_array, fall_array), axis=0)\n",
    "                except Exception as e:\n",
    "                    errors = errors + 1\n",
    "                    f = open(row['hex'] + \".txt\", \"a\")\n",
    "                    f.write(str(row))\n",
    "                    f.close()\n",
    "                    pass\n",
    "\n",
    "            X_array = np.append(X_array, [sample], axis=0)\n",
    "            labels.append(row.label)\n",
    "            hexes.append(row.hex)\n",
    "            fop.append(row.FIELD_OPERATION_GUID)\n",
    "            timestamp_tracking.append(timestamp_sample.copy())            \n",
    "\n",
    "            timestamp_sample.clear()\n",
    "            row_index = 0\n",
    "\n",
    "    print(f'***errors {errors}***')\n",
    "            \n",
    "    sorted_indices = np.argsort(hexes)\n",
    "    hexes = np.array(hexes)[sorted_indices]\n",
    "    labels = np.array(labels)[sorted_indices]\n",
    "    X_array = X_array[sorted_indices]\n",
    "    fop = np.array(fop)[sorted_indices]\n",
    "    timestamp_tracking = np.array(timestamp_tracking)[sorted_indices]\n",
    "    df = df.sort_values(by=['hex'])\n",
    "\n",
    "    return df, X_array, labels, hexes, fop, timestamp_tracking\n",
    "\n",
    "df, X_array, labels, hexes, fop, timestamp_tracking = generate_samples(df_positive, df_negative, sampling_method=sampling_method)\n",
    "\n",
    "print(X_array.shape)\n",
    "print(len(labels))\n",
    "print(len(hexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11bced",
   "metadata": {},
   "source": [
    "## TSNE Visualization BEFORE encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "if visualization_level > 2:\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model = TSNE(n_components=2, perplexity=4, random_state=0)\n",
    "    tsne_data = model.fit_transform(X_array.reshape(X_array.shape[0], number_of_bands * temporal_samples))\n",
    "    tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\"))\n",
    "    tsne_data = pd.concat([tsne_df, pd.DataFrame(labels, columns=['y'])], axis=1)\n",
    "    tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"y\"))\n",
    "\n",
    "    sns.jointplot(data=tsne_df, x=\"Dim_1\", y=\"Dim_2\", hue='y')\n",
    "    plt.show()\n",
    "    warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0233be4",
   "metadata": {},
   "source": [
    "## Sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7b5f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if visualization_level > 2:\n",
    "    sns.set(rc={'figure.figsize':(11,8)})\n",
    "\n",
    "    for i in range(0,3):\n",
    "        # This is what a positive sample looks like\n",
    "        xticklabels = range(1,13)\n",
    "        yticklabels = range(1,16)\n",
    "        yticklabels = timestamp_tracking[i]\n",
    "        ax = sns.heatmap(X_array[i], annot=False, cmap=\"viridis\", vmin=-2, vmax=2, \n",
    "                         xticklabels=xticklabels, yticklabels=yticklabels, \n",
    "                         fmt='g', annot_kws={\"fontsize\":24}, cbar=True)\n",
    "        ax.set_xlabel(\"Band\",fontsize=24)\n",
    "        ax.set_ylabel(\"Image date\",fontsize=24)\n",
    "        ax.set_title('Sample: ' + hexes[i], fontsize=26)\n",
    "        ax.tick_params(labelsize=20)\n",
    "\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=20)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(-4,-1):\n",
    "        xticklabels = range(1,13)\n",
    "        yticklabels = range(1,16)\n",
    "        yticklabels = timestamp_tracking[i]\n",
    "        ax = sns.heatmap(X_array[i], annot=False, cmap=\"rocket\", vmin=-2, vmax=2, \n",
    "                         xticklabels=xticklabels, yticklabels=yticklabels, \n",
    "                         fmt='g', annot_kws={\"fontsize\":20}, cbar=True)\n",
    "        ax.set_xlabel(\"Band\",fontsize=24)\n",
    "        ax.set_ylabel(\"Image date\",fontsize=24)\n",
    "        ax.set_title('Sample: ' + hexes[i], fontsize=26)\n",
    "        ax.tick_params(labelsize=20)\n",
    "\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=20)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b156ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "if visualization_level > 2:\n",
    "    value_vars = [i for i in df.columns if i.startswith('B')]\n",
    "    id_vars = ['label']\n",
    "    data = pd.melt(df, id_vars=id_vars, value_vars=value_vars)\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(11,8)})\n",
    "    sns.boxplot(x=\"variable\", y=\"value\", hue='label', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bde741",
   "metadata": {},
   "source": [
    "# Check correlation among bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization_level > 2:\n",
    "    sns.set_theme(style=\"white\")\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    sns.heatmap(corr,mask=mask,cmap=cmap, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455f13d",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total of samples:', X_array.shape)\n",
    "\n",
    "def prepare_training_dataset(test_index, val_index):\n",
    "#    labels_and_hexes = np.vstack((hexes, labels)).T\n",
    "\n",
    "    # Test dataset is the field we picked to test\n",
    "#    X_test = X_array[test_index]\n",
    "#    labels_test = np.array(labels)[test_index]\n",
    "#    hexes_test = np.array(hexes)[test_index]\n",
    "#    y_test = np.array(labels)[test_index]\n",
    "\n",
    "#    X_val = X_array[val_index]\n",
    "#    labels_val = np.array(labels)[val_index]\n",
    "#    hexes_val = np.array(hexes)[val_index]\n",
    "#    y_val = np.array(labels)[val_index]\n",
    "\n",
    "    # Train dataset is every but the test field\n",
    "    X_train = X_array[~(test_index+val_index )]\n",
    "    labels_train = np.array(labels)[~(test_index+val_index)]\n",
    "    hexes_train = np.array(hexes)[~(test_index+val_index)]\n",
    "\n",
    "    # Negative train\n",
    "    X_array_n = X_train[np.array(labels_train) == 0]\n",
    "    # Positive train\n",
    "    X_array_p = X_train[np.array(labels_train) == 1]\n",
    "\n",
    "    # Anchors are half of the positive train\n",
    "    X_array_anchors = X_array_p[0:math.floor((X_array_p.shape[0]/2))]\n",
    "    # Positives are the other half of the positive train\n",
    "    X_array_positives = X_array_p[math.floor(X_array_p.shape[0]/2):-1]\n",
    "    # Negatives are half of the negative train samples\n",
    "    X_array_negatives = X_array_n[0:math.floor((X_array_p.shape[0]/2))]\n",
    "\n",
    "    print('Total of anchors samples:', X_array_anchors.shape)\n",
    "    print('Total of positive samples:', X_array_positives.shape)\n",
    "    print('Total of negative samples:', X_array_negatives.shape)\n",
    "\n",
    "    samples_count = X_array_anchors.shape[0]\n",
    "    print('Number of samples:', samples_count)\n",
    "    \n",
    "    anchor_dataset = tf.data.Dataset\\\n",
    "        .from_tensor_slices(X_array_anchors)\n",
    "\n",
    "    positive_dataset = tf.data.Dataset\\\n",
    "        .from_tensor_slices(X_array_positives)\n",
    "    \n",
    "    negative_dataset = tf.data.Dataset\\\n",
    "        .from_tensor_slices(X_array_negatives)\n",
    "\n",
    "    return anchor_dataset, positive_dataset, negative_dataset, samples_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2eb3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize(anchor, positive, negative):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    axs = fig.subplots(3, 3)\n",
    "    for i in range(3):\n",
    "        show(axs[i, 0], anchor[i])\n",
    "        show(axs[i, 1], positive[i])\n",
    "        show(axs[i, 2], negative[i])\n",
    "\n",
    "\n",
    "#if visualization_level > 2:\n",
    "#    visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735316cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder architecture\n",
    "def get_encoder():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(temporal_samples, number_of_bands)),\n",
    "            layers.Conv1D(width, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Conv1D(width, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Conv1D(width, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Conv1D(width, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        \n",
    "        if loss_function_distance == 'euclidean':\n",
    "            ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "            an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        elif loss_function_distance == 'cosine':\n",
    "             # Normalize the embeddings\n",
    "            anchor = tf.nn.l2_normalize(anchor, axis=-1)\n",
    "            positive = tf.nn.l2_normalize(positive, axis=-1)\n",
    "            negative = tf.nn.l2_normalize(negative, axis=-1)\n",
    "\n",
    "            # Compute cosine similarities\n",
    "            ap_distance = 1.0 - tf.reduce_sum(anchor * positive, axis=-1, keepdims=True)\n",
    "            an_distance = 1.0 - tf.reduce_sum(anchor * negative, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            raise Exception('You must set a distance metric for the loss function')\n",
    "        \n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(temporal_samples,number_of_bands))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(temporal_samples,number_of_bands))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(temporal_samples,number_of_bands))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d2b2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_validation_dataset():\n",
    "\n",
    "    labeled_batch_size_ft = labeled_train_samples_ft\n",
    "    batch_size_ft = labeled_batch_size_ft\n",
    "\n",
    "    train_dataset_ft = tf.data.Dataset\\\n",
    "        .from_tensor_slices((X_train_ft, y_train_ft))\\\n",
    "        .shuffle(buffer_size=10 * labeled_batch_size_ft)\\\n",
    "        .batch(labeled_batch_size_ft, drop_remainder=True)\n",
    "\n",
    "    test_dataset_ft = tf.data.Dataset\\\n",
    "        .from_tensor_slices((X_test_ft))\\\n",
    "        .batch(batch_size_ft)\\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    validation_dataset_ft = tf.data.Dataset\\\n",
    "        .from_tensor_slices((X_val_ft, y_val_ft))\\\n",
    "        .batch(batch_size_ft)\\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    return batch_size_ft, train_dataset_ft, test_dataset_ft, validation_dataset_ft\n",
    "\n",
    "\n",
    "for i in range(0,len(df_positive.FIELD_OPERATION_GUID.unique()) + 1):\n",
    "    \n",
    "    \n",
    "    tid = str(uuid.uuid4()).split('-')[0]\n",
    "    \n",
    "    description = 'Training with test on field ' + str(i) + ' and validation on field ' + str(i+1) + ' and test identifier ' + tid\n",
    "    \n",
    "    field_index = i\n",
    "    test_field = df_positive.FIELD_OPERATION_GUID.unique()[field_index]\n",
    "    val_field = df_positive.FIELD_OPERATION_GUID.unique()[field_index+1]\n",
    "    print('Test Field = ', test_field)\n",
    "    print('Validation Field = ', val_field)\n",
    "    \n",
    "    test_index = np.array(fop) == test_field\n",
    "    val_index = np.array(fop) == val_field\n",
    "    print('The test field has number of hexes equals to ', sum(test_index))\n",
    "    \n",
    "    anchor_dataset, positive_dataset, negative_dataset, samples_count = prepare_training_dataset(test_index, val_index)\n",
    "    print(anchor_dataset, positive_dataset, negative_dataset)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "    dataset = dataset.shuffle(buffer_size=128)\n",
    "\n",
    "    # Let's now split our dataset in train and validation.\n",
    "    train_dataset = dataset.take(round(samples_count * 0.8))\n",
    "    val_dataset = dataset.skip(round(samples_count * 0.8))\n",
    "\n",
    "    train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "    #train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "    val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "    #val_dataset = val_dataset.prefetch(8)\n",
    "    \n",
    "    embedding = get_encoder()\n",
    "    \n",
    "    distances = DistanceLayer()(\n",
    "        embedding(anchor_input),\n",
    "        embedding(positive_input),\n",
    "        embedding(negative_input),\n",
    "    )\n",
    "    \n",
    "    siamese_network = Model(\n",
    "        inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    "    )\n",
    "        \n",
    "    siamese_model = SiameseModel(siamese_network)\n",
    "    siamese_model.compile(optimizer=keras.optimizers.Adam(0.0001), run_eagerly = False)\n",
    "    siamese_model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
    "    \n",
    "    \n",
    "    labels_and_hexes = np.vstack((hexes, labels)).T\n",
    "\n",
    "    test_index_ft = np.array(fop) == test_field\n",
    "    val_index_ft = np.array(fop) == val_field\n",
    "\n",
    "    X_test_ft = X_array[test_index_ft]\n",
    "    yl_test_ft = labels_and_hexes[test_index_ft]\n",
    "\n",
    "    X_val_ft = X_array[val_index_ft]\n",
    "    yl_val_ft = labels_and_hexes[val_index_ft]\n",
    "\n",
    "    X_train_ft = X_array[~(test_index_ft+val_index_ft)]\n",
    "    yl_train_ft = labels_and_hexes[~(test_index_ft+val_index_ft)]\n",
    "\n",
    "    \n",
    "    \n",
    "    labeled_train_samples_ft = X_train_ft.shape[0]\n",
    "    hexes_train_ft, y_train_ft = np.hsplit(yl_train_ft, 2)\n",
    "    hexes_test_ft, y_test_ft = np.hsplit(yl_test_ft, 2)\n",
    "    hexes_val_ft, y_val_ft = np.hsplit(yl_val_ft, 2)\n",
    "    fop_val_ft = fop[val_index_ft]\n",
    "    fop_test_ft = fop[test_index_ft]\n",
    "\n",
    "    y_val_ft = np.array(y_val_ft).T[0]\n",
    "    y_test_ft = np.array(y_test_ft).T[0]\n",
    "    y_train_ft = np.array(y_train_ft).T[0]\n",
    "    hexes_val_ft = np.array(hexes_val_ft).T[0]\n",
    "    hexes_test_ft = np.array(hexes_test_ft).T[0]\n",
    "    hexes_train_ft = np.array(hexes_train_ft).T[0]\n",
    "\n",
    "\n",
    "    y_train_ft = y_train_ft.astype(np.int)\n",
    "    y_test_ft = y_test_ft.astype(np.int)\n",
    "    y_val_ft = y_val_ft.astype(np.int)\n",
    "\n",
    "    print('Total of training samples:',X_train_ft.shape, len(y_train_ft))\n",
    "    print('Total of test samples:', X_test_ft.shape, len(y_test_ft))\n",
    "    print('Total of validation samples:', X_val_ft.shape, len(y_val_ft))\n",
    "    \n",
    "    batch_size_ft, train_dataset_ft, test_dataset_ft, validation_dataset_ft = prepare_validation_dataset()\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Supervised finetuning of the pretrained encoder\n",
    "    finetuning_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(temporal_samples, number_of_bands)),\n",
    "            embedding,\n",
    "            layers.Dense(1, activation='sigmoid'),\n",
    "        ],\n",
    "        name=\"finetuning_model\",\n",
    "    )\n",
    "    finetuning_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "    print(train_dataset_ft)\n",
    "    print(validation_dataset_ft)\n",
    "    finetuning_history = finetuning_model.fit(\n",
    "        train_dataset_ft, epochs=num_epochs, validation_data=validation_dataset_ft\n",
    "    )\n",
    "    print(\n",
    "        \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "            max(finetuning_history.history[\"val_acc\"]) * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #------------------------------------------------------------------------------------------\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score,confusion_matrix,matthews_corrcoef\n",
    "\n",
    "    print(\n",
    "        \"Finetuning maximal validation accuracy: {:.2f}%\".format(\n",
    "            max(finetuning_history.history[\"val_acc\"]) * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "    output = finetuning_model.predict(test_dataset_ft)\n",
    "    result = np.where(output > 0.5, 1, 0)\n",
    "\n",
    "    test_ft_df = pd.concat([\n",
    "                            pd.DataFrame(hexes_test_ft, columns=[\"hex\"]), \n",
    "                            pd.DataFrame(fop_test_ft, columns=[\"FIELD_OPERATION_GUID\"])\n",
    "                          ], axis=1)\n",
    "\n",
    "\n",
    "    display_hexes_map_v2(test_ft_df, filename=folder + 'test_actual_' + tid + '.png')\n",
    "\n",
    "    print('Triplet Loss Accuracy: {:.2f}%'.format(accuracy_score(y_test_ft, result) * 100))\n",
    "    print('Triplet Loss Recall: {:.2f}%'.format(recall_score(y_test_ft, result) * 100))\n",
    "    print('Triplet Loss Precision: {:.2f}%'.format(precision_score(y_test_ft, result) * 100))\n",
    "    print('Triplet Loss F1: {:.2f}%'.format(f1_score(y_test_ft, result) * 100))\n",
    "    m = confusion_matrix(y_test_ft, result)\n",
    "    print('Triplet Loss Acc by Class', (m.diagonal()/m.sum(axis=1)))\n",
    "    print('Triplet Loss MCC:', matthews_corrcoef(y_test_ft, result))\n",
    "\n",
    "    print(\n",
    "          '%.2f' % (float(accuracy_score(y_test_ft, result))*100) + '\\\\%&',\n",
    "          '%.2f' % float(f1_score(y_test_ft, result, average='weighted')*100)+ '\\\\%&',\n",
    "          '%.2f' % float(f1_score(y_test_ft, result, average=None)[0]*100)+ '\\\\%&',\n",
    "          '%.2f' % float(f1_score(y_test_ft, result, average=None)[1]*100)+ '\\\\%&',\n",
    "          '%.2f' % matthews_corrcoef(y_test_ft, result) + '\\\\\\\\',\n",
    "         )\n",
    "\n",
    "    print(confusion_matrix(y_test_ft, result))\n",
    "    confusion_matrix(y_test_ft, result)\n",
    "    accuracy_list = (y_test_ft==result.T[0])\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------------\n",
    "\n",
    "    df_classes_ft = pd.DataFrame(result, columns=['label'])\n",
    "    df_hexes_ft = pd.DataFrame(hexes_test_ft, columns=['hex'])\n",
    "    df_fop_ft = pd.DataFrame(fop_test_ft, columns=['FIELD_OPERATION_GUID'])\n",
    "\n",
    "    df_predict_ft = pd.concat([df_hexes_ft,df_classes_ft,df_fop_ft], axis=1)\n",
    "\n",
    "    display_hexes_map_v2(df_predict_ft, filename=folder + 'test_predicted_' + tid + '.png')\n",
    "    \n",
    "\n",
    "    # Test report\n",
    "\n",
    "    javascript = '<script> \\\n",
    "    function sortTable(n) { \\\n",
    "      var table, rows, switching, i, x, y, shouldSwitch, dir, switchcount = 0; \\\n",
    "      table = document.getElementById(\"myTable\"); \\\n",
    "      switching = true; \\\n",
    "      dir = \"asc\"; \\\n",
    "      while (switching) { \\\n",
    "        switching = false; \\\n",
    "        rows = table.rows; \\\n",
    "        for (i = 2; i < (rows.length - 1); i++) { \\\n",
    "          shouldSwitch = false; \\\n",
    "          x = rows[i].getElementsByTagName(\"TD\")[n]; \\\n",
    "          y = rows[i + 1].getElementsByTagName(\"TD\")[n]; \\\n",
    "          if (dir == \"asc\") { \\\n",
    "            if (x.innerHTML.toLowerCase() > y.innerHTML.toLowerCase()) { \\\n",
    "              shouldSwitch = true; \\\n",
    "              break; \\\n",
    "            } \\\n",
    "    } else if (dir == \"desc\") { \\\n",
    "    if (x.innerHTML.toLowerCase() < y.innerHTML.toLowerCase()) { \\\n",
    "    shouldSwitch = true; \\\n",
    "              break; \\\n",
    "            } \\\n",
    "          } \\\n",
    "        } \\\n",
    "        if (shouldSwitch) { \\\n",
    "          rows[i].parentNode.insertBefore(rows[i + 1], rows[i]); \\\n",
    "          switching = true; \\\n",
    "          switchcount ++; \\\n",
    "        } else { \\\n",
    "          if (switchcount == 0 && dir == \"asc\") { \\\n",
    "            dir = \"desc\"; \\\n",
    "            switching = true; \\\n",
    "          } \\\n",
    "        } \\\n",
    "      } \\\n",
    "    } \\\n",
    "    function myFunction() { \\\n",
    "      var input, filter, table, tr, td, i, txtValue; \\\n",
    "      input = document.getElementById(\"myInput\"); \\\n",
    "      filter = input.value.toUpperCase(); \\\n",
    "      table = document.getElementById(\"myTable\"); \\\n",
    "      tr = table.getElementsByTagName(\"tr\"); \\\n",
    "      var cols = document.getElementById(\"myTable\").rows[1].cells.length;\\\n",
    "      var displayLine = 0; \\\n",
    "      strs = filter.split(\"|\"); \\\n",
    "      for (row = 2; row < tr.length; row++) { \\\n",
    "        for (s = 0; s < strs.length; s++) { \\\n",
    "          for (c = 0; c < cols; c++) { \\\n",
    "            td = tr[row].getElementsByTagName(\"td\")[c]; \\\n",
    "            if (td) { \\\n",
    "              txtValue = td.textContent || td.innerText; \\\n",
    "              console.log(strs[s]); \\\n",
    "              if (txtValue.toUpperCase().indexOf(strs[s]) > -1) { \\\n",
    "                displayLine = displayLine + 1; \\\n",
    "              } else { \\\n",
    "                displayLine = displayLine; \\\n",
    "              } \\\n",
    "            } \\\n",
    "          } \\\n",
    "        } \\\n",
    "        if(displayLine >= (strs.length)) { \\\n",
    "            tr[row].style.display = \"\"; \\\n",
    "          } \\\n",
    "          else { \\\n",
    "            tr[row].style.display = \"none\"; \\\n",
    "          } \\\n",
    "          displayLine = 0; \\\n",
    "      } \\\n",
    "    }\\\n",
    "    function zoom(e){\\n\\\n",
    "      var zoomer = e.currentTarget;\\n\\\n",
    "      e.offsetX ? offsetX = e.offsetX : offsetX = e.touches[0].pageX\\n\\\n",
    "      e.offsetY ? offsetY = e.offsetY : offsetX = e.touches[0].pageX\\n\\\n",
    "      x = offsetX/zoomer.offsetWidth*100\\n\\\n",
    "      y = offsetY/zoomer.offsetHeight*100\\n\\\n",
    "      zoomer.style.backgroundPosition = x + \"% \" + y + \"%\";\\n\\\n",
    "    }\\n\\\n",
    "    </script>'\n",
    "\n",
    "    style = '<style>\\\n",
    "    * {\\\n",
    "      box-sizing: border-box;\\\n",
    "    }\\\n",
    "    .tooltip {\\\n",
    "      position: relative;\\\n",
    "      display: inline-block;\\\n",
    "      border-bottom: 1px dotted black;\\\n",
    "    }\\\n",
    "    .tooltip .tooltiptext {\\\n",
    "      visibility: hidden;\\\n",
    "      width: 240px;\\\n",
    "      background-color: black;\\\n",
    "      color: #fff;\\\n",
    "      text-align: center;\\\n",
    "      border-radius: 6px;\\\n",
    "      padding: 5px 0;\\\n",
    "      position: absolute;\\\n",
    "      z-index: 1;\\\n",
    "    }\\\n",
    "    .tooltip:hover .tooltiptext {\\\n",
    "      visibility: visible;\\\n",
    "    }\\\n",
    "    #myInput {\\\n",
    "      background-image: url(\"/css/searchicon.png\");\\\n",
    "      background-position: 10px 10px;\\\n",
    "      background-repeat: no-repeat;\\\n",
    "      width: 100%;\\\n",
    "      font-size: 16px;\\\n",
    "      padding: 12px 20px 12px 40px;\\\n",
    "      border: 1px solid #ddd;\\\n",
    "      margin-bottom: 12px;\\\n",
    "    }\\\n",
    "    #myTable {\\\n",
    "      border-collapse: collapse;\\\n",
    "      width: 100%;\\\n",
    "      border: 1px solid #ddd;\\\n",
    "      font-size: 18px;\\\n",
    "    }\\\n",
    "    #myTable th, #myTable td {\\\n",
    "      text-align: left;\\\n",
    "      padding: 12px;\\\n",
    "    }\\\n",
    "    #myTable tr {\\\n",
    "      border-bottom: 1px solid #ddd;\\\n",
    "    }\\\n",
    "    #myTable tr.header{\\\n",
    "      background-color: #f1f1f1;\\\n",
    "      cursor: pointer;\\\n",
    "    }\\\n",
    "    #myTable tr:hover {\\\n",
    "      background-color: #f1f1f1;\\\n",
    "    }\\\n",
    "    figure.zoom {\\\n",
    "      background-position: 50% 50%;\\\n",
    "      position: relative;\\\n",
    "      width: 450px;\\\n",
    "      overflow: hidden;\\\n",
    "      cursor: zoom-in;\\\n",
    "    }\\\n",
    "    figure.zoom img:hover {\\\n",
    "      opacity: 0;\\\n",
    "    }\\\n",
    "    figure.zoom img {\\\n",
    "      transition: opacity 0.5s;\\\n",
    "      display: block;\\\n",
    "      width: 100%;\\\n",
    "      height: 100%;\\\n",
    "    }\\\n",
    "    </style>'\n",
    "\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        with open(folder + 'report.html', 'r') as file:\n",
    "            table = file.read()\n",
    "    except:\n",
    "        table = ''\n",
    "        table = '<html><head>'\n",
    "        table += javascript\n",
    "        table += '<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">'\n",
    "        table += style\n",
    "        table += '</head><body><h2>Search</h2>'\n",
    "        table += '<input type=\"text\" id=\"myInput\" onkeyup=\"myFunction()\" placeholder=\"Search for names..\" title=\"Type in a name\">'\n",
    "        table += '<table border=1 id=\"myTable\">\\n'\n",
    "        # Create the table's column headers\n",
    "        table += '<tr>\\n'\n",
    "        table += '<th colspan=6>Parameters</th>\\n'\n",
    "        table += '<th colspan=1>Validation Results</th>'\n",
    "        table += '<th colspan=6>Test Results</th>'\n",
    "        table += '</tr>\\n'\n",
    "\n",
    "        table += '<tr>\\n'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(0)\"><div class=\"tooltip\">Description<span class=\"tooltiptext\">Experiment description</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(1)\"><div class=\"tooltip\">width<span class=\"tooltiptext\">Width of encoded vector</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(2)\"><div class=\"tooltip\">margin<span class=\"tooltiptext\">Margin of triplet siamese algorithm</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(3)\"><div class=\"tooltip\">loss function<span class=\"tooltiptext\">Distance used in loss function</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(4)\"><div class=\"tooltip\">sampling method<span class=\"tooltiptext\">Method to create the sample matrix</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(5)\"><div class=\"tooltip\">Bands<span class=\"tooltiptext\">Sentinel-2 bands utilized</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(6)\"><div class=\"tooltip\">Accuracy<span class=\"tooltiptext\">Validation accuracy</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(7)\"><div class=\"tooltip\">Accuracy<span class=\"tooltiptext\">Test accuracy</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(8)\"><div class=\"tooltip\">F1<span class=\"tooltiptext\">F1 weighted</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(9)\"><div class=\"tooltip\">MCC<span class=\"tooltiptext\">Matthews Correlation Coeficient</span></div></th>'\n",
    "        table += '<th href=\"javascript:void(0)\" onclick=\"sortTable(10)\"><div class=\"tooltip\">Confusion Matrix<span class=\"tooltiptext\">Confusion Matrix</span></div></th>'\n",
    "        table += '<th>Actual</th>'\n",
    "        table += '<th>Predicted</th>'\n",
    "        table += '</tr>\\n'\n",
    "        print('No report file found')\n",
    "\n",
    "\n",
    "    fileout = open(folder + \"report.html\", \"w\")\n",
    "\n",
    "    table += '<tr>\\n'\n",
    "    table += f'<td>{description}</td><td>{width}</td><td>{siamese_margin}</td><td>{loss_function_distance}</td><td>{sampling_method}</td><td>{band_features}</td>'\n",
    "    table += '<td>{:.2f}%</td>'.format(max(finetuning_history.history[\"val_acc\"]) * 100)\n",
    "    table += '<td>%.2f'  % (float(accuracy_score(y_test_ft, result))*100) + '% </td>'\n",
    "    table += '<td>%.2f'  % float(f1_score(y_test_ft, result, average='weighted')*100) + '% </td>'\n",
    "    table += '<td>%.2f' % matthews_corrcoef(y_test_ft, result)+ '</td>'\n",
    "    table += '<td>' + str(m[0]) + '<br />' + str(m[1]) + '</td>'\n",
    "    table += f'<td><figure class=\"zoom\" onmousemove=\"zoom(event)\" style=\"background-image: url(test_actual_{tid}.png)\"> <img src=\"test_actual_{tid}.png\" /> </figure></td>'\n",
    "    table += f'<td><figure class=\"zoom\" onmousemove=\"zoom(event)\" style=\"background-image: url(test_predicted_{tid}.png)\"> <img src=\"test_predicted_{tid}.png\" /> </figure></td>'\n",
    "    table += '</tr>\\n'\n",
    "\n",
    "    fileout.writelines(table)\n",
    "    fileout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcf657",
   "metadata": {},
   "source": [
    "# Finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3127552",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Total of samples:', X_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a91cf8",
   "metadata": {},
   "source": [
    "## Append results to Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bfe0e",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321811eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
