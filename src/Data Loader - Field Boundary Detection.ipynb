{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf20650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: typing_extensions; python_version < \"3.8\" in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from seaborn) (4.4.0)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from pandas>=0.25->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\en89912\\pycharmprojects\\fieldboundarydetection\\venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.25->seaborn) (1.16.0)\n",
      "Collecting tensorflow_io\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/67/b1fd944186f578a2798affdae894637a7b8b6d7d1da607e270f552661855/tensorflow_io-0.29.0-cp37-cp37m-win_amd64.whl (22.9MB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.29.0 (from tensorflow_io)\n",
      "  Downloading https://files.pythonhosted.org/packages/50/5d/57d8c4f9bfa561f39427302df6194a15a987858b882ed19989e92ca28651/tensorflow_io_gcs_filesystem-0.29.0-cp37-cp37m-win_amd64.whl (1.5MB)\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
      "  Found existing installation: tensorflow-io-gcs-filesystem 0.27.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.27.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.27.0\n",
      "Successfully installed tensorflow-io-0.29.0 tensorflow-io-gcs-filesystem-0.29.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Install packages required only once\n",
    "install_packages = True\n",
    "\n",
    "if install_packages:\n",
    "    !{sys.executable} -m pip install sklearn\n",
    "    !{sys.executable} -m pip install seaborn\n",
    "    !{sys.executable} -m pip install tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bbc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dir_paths, batch_size, num_bands=12, shuffle_buffer=200000, crop_season_only=False,\n",
    "                           num_hexes_per_field=None, min_imgs=10, max_imgs=30, predict_on_N_imgs=None, perc_field_fill=0.8,\n",
    "                           min_num_hexes_per_field=50):\n",
    "\n",
    "    '''\n",
    "    :param dir_path: path of the field op guid partition\n",
    "    :param max_imgs: Must match the model input size\n",
    "    :param num_bands: sentinel 2 = 12 bands\n",
    "    :param flatten_output: True for dense models\n",
    "    :param get_full_timseries_output: only for investigations\n",
    "    :param predict_on_N_imgs: number of images to use for prediction.  Like max_imgs, it must match the model training\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    mean_band_vals = tf.constant([1256,1396,1601,1757,2066,2822,3279,3410,3491,3570,2573,1868], tf.float32)\n",
    "    std_band_vals = tf.constant([2199,2247,2171,2233,2204,1916,1891,1907,1777,1816,1151,1079], tf.float32)\n",
    "\n",
    "\n",
    "    def parse(row, fop):\n",
    "        tf.assert_greater(row['L14_RSum_harvest'], tf.constant(0.7, tf.float64))\n",
    "\n",
    "        split_scenes = tf.strings.split(tf.reshape(row['scene_ids'],[-1]),',')[0]\n",
    "        # if tf.strings.length(split_scenes[-1]) != 60: split_scenes = split_scenes[:-1]\n",
    "\n",
    "        ################### remove duplicate scenes by randomly choosing one of the duplicates #########################\n",
    "        _, _, scene_counts = tf.unique_with_counts(split_scenes)\n",
    "        unique_idxs = tf.concat([[0],tf.cumsum(scene_counts)[:-1]],axis=0)\n",
    "\n",
    "        def rand_sample(maxval):\n",
    "            return tf.random.uniform(shape=(), minval=0, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "        unique_idxs = unique_idxs + tf.map_fn(fn=rand_sample, elems=scene_counts, fn_output_signature=tf.int32)\n",
    "        ###################### constrain to crop season only ###########################################################\n",
    "        def convert_bytes_dates(idx):\n",
    "            # img_date_bytes = tf.strings.substr(row['img_dates_int'],idx*2,2)  # %band_lengths guarantees idx is in range\n",
    "            img_date_bytes = tf.strings.substr(row['img_dates'],idx*2,2)  # %band_lengths guarantees idx is in range\n",
    "            img_date_ints = tf.io.decode_raw(img_date_bytes, out_type=tf.uint16, little_endian=False, fixed_length=2)\n",
    "            return tf.cast(img_date_ints, tf.int32)\n",
    "\n",
    "        img_dates = tf.map_fn(fn=convert_bytes_dates, elems=unique_idxs, fn_output_signature=tf.int32)\n",
    "        row['seeding_date'] = tf.cast(tf.cast(row['seeding_date'], tf.float32)/24., tf.int32)\n",
    "        row['harvest_date'] = tf.cast(tf.cast(row['harvest_date'], tf.float32)/24., tf.int32)\n",
    "        if crop_season_only:\n",
    "            crop_season_mask = tf.reshape(tf.math.logical_and(img_dates < row['harvest_date'], img_dates > row['seeding_date']), shape=(-1,))\n",
    "            unique_idxs = tf.boolean_mask(unique_idxs, crop_season_mask)\n",
    "\n",
    "        r_samp_idxs = unique_idxs\n",
    "\n",
    "        img_dates = tf.map_fn(fn=convert_bytes_dates, elems=r_samp_idxs, fn_output_signature=tf.int32)\n",
    "\n",
    "        def convert_bytes(idx):\n",
    "            random_img_bytes = tf.strings.substr(row['bands'],idx*num_bands*2,num_bands*2)  # %band_lengths guarantees idx is in range\n",
    "            random_img_ints = tf.io.decode_raw(random_img_bytes, out_type=tf.uint16, little_endian=False, fixed_length=24)\n",
    "            return (tf.cast(random_img_ints, tf.float32) - mean_band_vals)/std_band_vals\n",
    "\n",
    "        band_vals = tf.map_fn(fn=convert_bytes, elems=r_samp_idxs, fn_output_signature=tf.float32)\n",
    "\n",
    "        def convert_bytes_NDVI(idx):\n",
    "            RED = tf.cast(tf.io.decode_raw(tf.strings.substr(row['bands'], idx * num_bands * 2 + 6, 2), out_type=tf.uint16, little_endian=False, fixed_length=2), tf.float32)\n",
    "            NIR = tf.cast(tf.io.decode_raw(tf.strings.substr(row['bands'], idx * num_bands * 2 + 14, 2), out_type=tf.uint16, little_endian=False, fixed_length=2), tf.float32)\n",
    "            return (NIR - RED)/(NIR + RED)\n",
    "\n",
    "        NDVI = tf.map_fn(fn=convert_bytes_NDVI, elems=r_samp_idxs, fn_output_signature=tf.float32)\n",
    "        return band_vals, tf.cast(row['mean_YieldVolumePerArea_bu_per_ha'], tf.float32), tf.squeeze(NDVI), tf.squeeze(img_dates), row['seeding_date'], row['harvest_date'], fop\n",
    "\n",
    "\n",
    "    def tf_random_choice(logits, K):\n",
    "        z = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits), 0, 1)))\n",
    "        _, indices = tf.nn.top_k(logits + z, K)\n",
    "        return indices\n",
    "\n",
    "\n",
    "    def image_selector_proc_fn(*input_val):\n",
    "        band_vals, y, NDVI_vals, img_dates, seeding_dates, harvest_dates, fop = input_val\n",
    "        tf.Assert(tf.shape(band_vals)[0] > min_num_hexes_per_field, [1])\n",
    "        unique_cnts = tf.unique_with_counts(tf.reshape(img_dates, [-1]))\n",
    "        unique_dates = tf.sort(tf.boolean_mask(unique_cnts.y, tf.logical_and(unique_cnts.y > 0, unique_cnts.count / tf.shape(img_dates)[0] > perc_field_fill)))\n",
    "        harvest_date = tf.unique_with_counts(tf.reshape(harvest_dates, [-1]))\n",
    "        harvest_date = harvest_date.y[tf.argmax(harvest_date.count)]\n",
    "        seeding_date = tf.unique_with_counts(tf.reshape(seeding_dates, [-1]))\n",
    "        seeding_date = seeding_date.y[tf.argmax(seeding_date.count)]\n",
    "        ### tf.map_fn each \"column\" (date) of NDVI values to get R**2 from linear_reg (only for in-season dates -- out-of-season label as zero)\n",
    "        def linear_reg(date_val):\n",
    "            NDVI = tf.reshape(tf.gather_nd(NDVI_vals, tf.where(img_dates == date_val)), [-1,1])\n",
    "            y_ = tf.reshape(tf.gather_nd(y, tf.where(img_dates == date_val)[:,0:1]), [-1,1])\n",
    "            lst_sq_fit = tf.linalg.lstsq(tf.concat([tf.ones_like(NDVI), NDVI], axis=1), y_)\n",
    "            y_pred_lst_sq_corrected = tf.matmul(tf.concat([tf.ones_like(NDVI), NDVI], axis=1), lst_sq_fit)\n",
    "            unexplained_error = tf.reduce_sum(tf.square(y_ - y_pred_lst_sq_corrected))\n",
    "            total_error = tf.reduce_sum(tf.square(y_ - tf.reduce_mean(y_)))\n",
    "            R_squared = 1. - unexplained_error / total_error\n",
    "            return R_squared\n",
    "\n",
    "        if not crop_season_only:\n",
    "            crop_season_mask = tf.reshape(tf.math.logical_and(unique_dates < tf.reshape(harvest_date, [-1, 1]), unique_dates > tf.reshape(seeding_date, [-1, 1])), [-1])\n",
    "            Rsq_vals = tf.map_fn(linear_reg, tf.boolean_mask(unique_dates, crop_season_mask), fn_output_signature=tf.float32)\n",
    "            Rsq_vals = tf.pad(Rsq_vals, [[tf.where(unique_dates > seeding_date)[0][0],0]])\n",
    "            Rsq_vals = tf.pad(Rsq_vals, [[0, len(unique_dates) - len(Rsq_vals)]])\n",
    "        else:\n",
    "            Rsq_vals = tf.map_fn(linear_reg, unique_dates, fn_output_signature=tf.float32)\n",
    "\n",
    "        '''\n",
    "        if estimating image regression quality:\n",
    "        -- need only subset of hexes for field (e.g. 10)\n",
    "        -- random dates for each hex selected\n",
    "        \n",
    "        if encoding time series:\n",
    "        -- need entire field\n",
    "        -- use consistent dates (same across all hexes for that field)\n",
    "        '''\n",
    "        # Regression Quality Estimation\n",
    "        if num_hexes_per_field:\n",
    "            num_hexes_per_field_ = tf.minimum(num_hexes_per_field, min_num_hexes_per_field)\n",
    "            rsq_max = tf.reduce_max(Rsq_vals)\n",
    "            def regression_quality_sampling(idx):\n",
    "                if predict_on_N_imgs:\n",
    "                    r_samp_len = tf.minimum(tf.minimum(predict_on_N_imgs, max_imgs), len(unique_dates))\n",
    "                else:\n",
    "                    r_samp_len = tf.random.uniform(shape=(), minval=tf.minimum(min_imgs, len(unique_dates)),\n",
    "                                                   maxval=tf.minimum(max_imgs, len(unique_dates)) + 1, dtype=tf.int32)\n",
    "\n",
    "                r_samp_dates = tf.sort(tf.random.shuffle(unique_dates)[:r_samp_len])\n",
    "                r_samp_idxs = tf.where(img_dates[idx] == tf.reshape(r_samp_dates, [-1, 1]))[:, -1]\n",
    "                r_samp_dates = tf.sort(tf.sets.intersection(r_samp_dates[None, :], tf.gather(img_dates[idx], r_samp_idxs)[None, :]).values)\n",
    "                r_samp_len = len(r_samp_idxs)\n",
    "                gt_rsq = tf.gather(Rsq_vals, tf.where(unique_dates == tf.reshape(r_samp_dates, [-1, 1]))[:, -1])\n",
    "                gt_rsq = tf.pad(gt_rsq, [[max_imgs - r_samp_len, 0]])\n",
    "\n",
    "                img_dates_out = tf.gather_nd(img_dates[idx], tf.reshape(r_samp_idxs, [-1,1]))\n",
    "                img_diffs = tf.cast(img_dates_out[-1] - img_dates_out, tf.float32) / 365\n",
    "                band_vals_out = tf.gather_nd(band_vals[idx], tf.reshape(r_samp_idxs, [-1,1]))\n",
    "                band_vals_out = tf.concat([band_vals_out, tf.reshape(img_diffs, (-1, 1))], axis=1)\n",
    "                band_vals_out = tf.pad(band_vals_out, [[(max_imgs - r_samp_len), 0], [0, 0]])\n",
    "                band_vals_out = tf.reshape(band_vals_out, [-1])\n",
    "\n",
    "                return  band_vals_out, gt_rsq/rsq_max\n",
    "\n",
    "            random_hex_idxs = tf.random.uniform(shape=[num_hexes_per_field_], minval=0, maxval=len(img_dates), dtype=tf.int32)\n",
    "            band_vals_out_, gt_rsq_out_ = tf.map_fn(regression_quality_sampling, random_hex_idxs, fn_output_signature=(tf.float32, tf.float32))\n",
    "\n",
    "            return band_vals_out_, gt_rsq_out_\n",
    "\n",
    "        # encode time series of imagery\n",
    "        else:\n",
    "            tf.Assert(crop_season_only, [1])\n",
    "            ## one random sample of dates for all fields\n",
    "            if predict_on_N_imgs:\n",
    "                r_samp_len = tf.minimum(tf.minimum(predict_on_N_imgs, max_imgs), len(unique_dates))\n",
    "            else:\n",
    "                r_samp_len = tf.random.uniform(shape=(), minval=tf.minimum(min_imgs, len(unique_dates)), maxval=tf.minimum(max_imgs, len(unique_dates)) + 1, dtype=tf.int32)\n",
    "            r_samp_dates = tf.sort(tf.random.shuffle(unique_dates)[:r_samp_len])\n",
    "            ## boolean mask keep dims by substituting zeros\n",
    "            # def get_imgs(date_val):\n",
    "            #     # tf.where(array, tensor, tf.zeros_like(tensor))\n",
    "            #     return tf.where(tf.tile(tf.expand_dims(img_dates == date_val, axis=2), [1,1,tf.shape(band_vals)[-1]]), band_vals, tf.zeros_like(band_vals))\n",
    "            #\n",
    "            # band_vals_out = tf.map_fn(get_imgs, r_samp_dates, fn_output_signature=tf.float32)\n",
    "            # band_vals_out = tf.reduce_sum(band_vals_out, axis=0)\n",
    "\n",
    "            def embedding_time_series(idx):\n",
    "                r_samp_idxs = tf.where(img_dates[idx] == tf.reshape(r_samp_dates, [-1, 1]))[:, -1]\n",
    "                r_samp_len = len(r_samp_idxs)\n",
    "\n",
    "                img_dates_out = tf.gather_nd(img_dates[idx], tf.reshape(r_samp_idxs, [-1, 1]))\n",
    "                img_diffs = tf.cast(img_dates_out[-1] - img_dates_out, tf.float32) / 365\n",
    "                band_vals_out = tf.gather_nd(band_vals[idx], tf.reshape(r_samp_idxs, [-1, 1]))\n",
    "                band_vals_out = tf.concat([band_vals_out, tf.reshape(img_diffs, (-1, 1))], axis=1)\n",
    "                band_vals_out = tf.pad(band_vals_out, [[(max_imgs - r_samp_len), 0], [0, 0]])\n",
    "                band_vals_out = tf.reshape(band_vals_out, [-1])\n",
    "\n",
    "                return band_vals_out\n",
    "\n",
    "            band_vals_out_ = tf.map_fn(embedding_time_series, tf.range(len(band_vals)), fn_output_signature=tf.float32)\n",
    "\n",
    "            ## best Rsq of dates sampled (not global across all dates available for a give field.  If that is desired, use Rsq_max under regression quality switch above\n",
    "            rsq_max = tf.reduce_max(tf.gather(Rsq_vals, tf.where(unique_dates == tf.reshape(r_samp_dates, [-1, 1]))[:, -1]))\n",
    "            return band_vals_out_, y, rsq_max\n",
    "\n",
    "\n",
    "    def parquet_ds(field_op_dir):\n",
    "        ds = tf.data.Dataset.list_files(field_op_dir + '/*.parquet')\n",
    "        ds = ds.interleave(lambda x: tfio.IODataset.from_parquet(x, parquet_dict), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "        ds = ds.map(partial(parse, fop=tf.strings.split(field_op_dir, 'FIELD_OPERATION_GUID=')[-1]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        # return ds.apply(tf.data.experimental.ignore_errors()).padded_batch(10000, ([None, 12], [], [], [None], [None]))\n",
    "        return ds.apply(tf.data.experimental.ignore_errors()).padded_batch(10000, ([None, 12], [], [None], [None], [], [], []))\n",
    "               # return ds.apply(tf.data.experimental.ignore_errors()).padded_batch(10000, ([None, 12], [], [None], [None]))\n",
    "\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dir_paths).shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "    ds = ds.interleave(parquet_ds, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.map(image_selector_proc_fn).apply(tf.data.experimental.ignore_errors())\n",
    "    if num_hexes_per_field:\n",
    "        ds = ds.batch(batch_size)\n",
    "    else:\n",
    "        ds = ds.padded_batch(batch_size, padded_shapes=([None, (num_bands + 1)*max_imgs], [None], []), padding_values=-1.)\n",
    "    # ds = ds.padded_batch(batch_size, padded_shapes=([None, None, 12], [None], [None], [None, None], [None, None]))\n",
    "    # ds = ds.padded_batch(batch_size, padded_shapes=([None, None, 12], [None], [None, None], [None, None], [None], [None]))\n",
    "    # ds = ds.map(image_selector_proc_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).apply(tf.data.experimental.ignore_errors())\n",
    "\n",
    "    return ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
